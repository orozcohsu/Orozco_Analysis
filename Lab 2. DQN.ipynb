{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 2. DQN.ipynb","version":"0.3.2","provenance":[{"file_id":"10xa1-vUnB2M3tofVOkvO60YuBYgEwMro","timestamp":1544249141447}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"BAtRJ0e_0R2J","colab_type":"text"},"cell_type":"markdown","source":["# Lab 2. 利用Deep Q-Network訓練\n","\n","大家好。歡迎各位再度回來增強式學習的世界。這一次，我們要更進化一點，利用深度網路來作為Q Function，達到比用Q-Table更好的效果。\n","\n","首先，因為我們使用的是Google CoLab，為了能夠方便在這個環境上面顯示，我們先安裝一些必要的套件。"]},{"metadata":{"id":"1dkImtaLIVx5","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U027GmvlIhM3","colab_type":"code","outputId":"e3c0cac5-cc6a-410e-f702-339005fad4c7","executionInfo":{"status":"ok","timestamp":1544250882480,"user_tz":-480,"elapsed":651,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh4.googleusercontent.com/-qMox6ymzUwc/AAAAAAAAAAI/AAAAAAAAACU/55qpbMUgwG4/s64/photo.jpg","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["from IPython import display as ipythondisplay\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1024, 768))\n","display.start()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"Spe4q7W-UXIx","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qO-5TeCLUYNE","colab_type":"text"},"cell_type":"markdown","source":["這一個實驗，我們來玩一下小車車爬山坡的遊戲。這個遊戲的環境在這裡：https://github.com/openai/gym/wiki/MountainCar-v0"]},{"metadata":{"id":"dd8ki05DVDsQ","colab_type":"text"},"cell_type":"markdown","source":["# 亂數策略玩遊戲\n","首先，我們還是用亂走來玩一下這個遊戲，看看會發生什麼事情。"]},{"metadata":{"id":"1XjcEQ5iUwHP","colab_type":"code","outputId":"93ad75b5-d828-4c1d-a06f-042b52f9c59e","executionInfo":{"status":"ok","timestamp":1544250898030,"user_tz":-480,"elapsed":18,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh4.googleusercontent.com/-qMox6ymzUwc/AAAAAAAAAAI/AAAAAAAAACU/55qpbMUgwG4/s64/photo.jpg","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":367}},"cell_type":"code","source":["env = gym.make(\"MountainCar-v0\")\n","env.reset()\n","prev_screen = env.render(mode='rgb_array')\n","plt.imshow(prev_screen)\n","\n","rewards=0\n","for t in range(200):\n","  #亂數選一個動作\n","  action = env.action_space.sample()\n","  obs, reward, done, info = env.step(action)\n","  #print(\"reward:\",reward,\"done:\",done)\n","  \n","  rewards+=reward\n","  #env.render()用來顯示結果\n","  screen = env.render(mode='rgb_array')\n","  #用matploblib畫出來\n","  if t%10==0:\n","      plt.imshow(screen)\n","      ipythondisplay.clear_output(wait=True)\n","      ipythondisplay.display(plt.gcf())\n","    \n","  if done:\n","\t  break  \n","\n","ipythondisplay.clear_output(wait=True)\n","env.close()\n","\n","print(\"Total reward:\",rewards)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Total reward: -200.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAe0AAAFMCAYAAADm9OSwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f44be0d4c18>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"s2Y7aOYsWhJL","colab_type":"text"},"cell_type":"markdown","source":["好慘啊！爬不上去！\n","\n","看一下reward，每走一步扣一分，直到position是0.5的時後才算是到達。\n","\n","所以我們走了兩百步，他就得到了-200分....\n","\n","這個環境跟CartPole比起來複雜很多，因為不管怎樣走，都是扣分。"]},{"metadata":{"id":"xLr_R5gGXta3","colab_type":"text"},"cell_type":"markdown","source":["# Quiz:\n","想試試看自己手刻一個策略以及利用Q-Learning來試試看嗎？你的策略會讓他更高分嗎？\n"]},{"metadata":{"id":"hBQGfgdYX84j","colab_type":"text"},"cell_type":"markdown","source":["# Deep Q-Network\n","\n","如果有做這個Quiz的朋友們，你一定發現一件事情。\n","跟cartpole一樣，這一個state也是連續的。我們可以用設定bucket的方法來做，不過總覺得有點人工智慧。因此，在這一個情境之中，我們來試試看利用深度網路的技巧來學習一個Q函數。\n","\n","## DQN class\n","\n","我們要將DQN做成一個class。這個class裡面有許多的method要實作。以下我們逐一介紹。"]},{"metadata":{"id":"AcOWsWlYfgN-","colab_type":"text"},"cell_type":"markdown","source":["### DQN model\n","\n","這邊我們用來建置我們要用的深度網路。\n","* 輸入：state的向量\n","* 輸出：action的向量\n","* 網路結構：先用幾層簡單的full-connected layer來試試看 \n","* loss：mean squared error (因為是比較兩個向量的差)\n","\n","```\n","def create_model(self):\n","        model   = Sequential()\n","        state_shape  = self.env.observation_space.shape\n","        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n","        model.add(Dense(48, activation=\"relu\"))\n","        model.add(Dense(24, activation=\"relu\"))\n","        model.add(Dense(self.env.action_space.n))\n","        model.compile(loss=\"mean_squared_error\",\n","            optimizer=Adam(lr=self.learning_rate))\n","        return model\n","```\n","\n","\n"]},{"metadata":{"id":"JYg98fryb5HC","colab_type":"text"},"cell_type":"markdown","source":["### 初始化\n","\n","這邊我們會設定一些hyper-parameters以及兩個model。\n","\n","\n","```\n","def __init__(self, env):\n","        self.env     = env\n","        self.memory  = deque(maxlen=2000)\n","        \n","        self.gamma = 0.95\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.995\n","        self.learning_rate = 0.01\n","        self.tau = .05\n","        self.model = self.create_model()\n","        # \"hack\" implemented by DeepMind to improve convergence\n","        self.target_model = self.create_model()\n","```\n","\n","hyper-parameter的意義如下：\n","*   env: 就是我們要玩的遊戲的env變數。\n","*   memory: 每一次的嘗試都記在這裡，這裡的資料被記下來，然後亂數的取出來，當作深度網路的訓練資料。這個樣做是為了避免訓練資料中的環境資料的偏差而造成預測失準。\n","* gamma: 獎勵decay的比例 $\\gamma$\n","* epsilon和epsilon_decay: 用來控制RL在現今的狀態是著重探索(explore)還是利用收集到的資訓(exploit)。\n","\n","model有兩個。一個是model，另外一個是target_model。為什麼要有兩個model的原因，是因為我們在訓練的時候是每一步都會訓練到我們的model。我們會讓兩個模型有較不同的訓練速度，一個比較快，一個比較慢。如此一來才可以讓我們的model最後能收斂。"]},{"metadata":{"id":"4ukEylx_jZSI","colab_type":"text"},"cell_type":"markdown","source":["### DQN Training\n","DQN Training分成三個步驟：記憶, 學習, 以及 重新定向目標。\n","\n","1. 首先，記憶部分滿簡單的，基本上就是把我們這次的嘗試的資訊記在memory中：\n","\n","```\n","def remember(self, state, action, reward, new_state, done):\n","        self.memory.append([state, action, reward, new_state, done])\n","```\n","\n","2. 接下來是學習部分\n","\n","在此處，我們先用亂數從memory中抽出一些記錄來進行學習。\n","\n","對於取出的每一筆紀錄來說，我們逐步更新*model*。在這邊值得注意的是，我們會利用*target_model*針對目前的狀態預測target以及未來可能得到的reward，然後利用公式算出這一個紀錄的action得到的reward。然後再更新*model*。\n","\n","```\n","def replay(self):\n","        batch_size = 32\n","        if len(self.memory) < batch_size: \n","            return\n","        samples = random.sample(self.memory, batch_size)\n","        for sample in samples:\n","            state, action, reward, new_state, done = sample\n","            target = self.target_model.predict(state)\n","            if done:\n","                target[0][action] = reward\n","            else:\n","                Q_future = max(\n","                    self.target_model.predict(new_state)[0])\n","                target[0][action] = reward + Q_future * self.gamma\n","            self.model.fit(state, target, epochs=1, verbose=0)\n","```\n","\n","3. 重新定向目標\n","這個地方基本上就是拷貝在*model*中的值，這個部份更新的速度會比較不頻繁，來達到上述兩種不同速度模型更新的目標。\n","\n","\n","```\n","def target_train(self):\n","        weights = self.model.get_weights()\n","        target_weights = self.target_model.get_weights()\n","        for i in range(len(target_weights)):\n","            target_weights[i] = weights[i]\n","        self.target_model.set_weights(target_weights)\n","```\n","\n","\n","\n","\n"]},{"metadata":{"id":"yjJUg3huwkAl","colab_type":"text"},"cell_type":"markdown","source":["## 完整的程式碼"]},{"metadata":{"id":"Xp5b0EU3GewU","colab_type":"code","outputId":"2f55c0dd-fa45-42ce-95da-3b17682376e5","executionInfo":{"status":"ok","timestamp":1544251514232,"user_tz":-480,"elapsed":606440,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh4.googleusercontent.com/-qMox6ymzUwc/AAAAAAAAAAI/AAAAAAAAACU/55qpbMUgwG4/s64/photo.jpg","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":974}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import random\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import Adam\n","\n","from collections import deque\n","\n","class DQN:\n","    def __init__(self, env):\n","        self.env     = env\n","        self.memory  = deque(maxlen=2000)\n","        \n","        self.gamma = 0.85\n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.01\n","        self.epsilon_decay = 0.995\n","        self.learning_rate = 0.005\n","        self.tau = .125\n","\n","        self.model        = self.create_model()\n","        self.target_model = self.create_model()\n","\n","    def create_model(self):\n","        model   = Sequential()\n","        state_shape  = self.env.observation_space.shape\n","        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n","        model.add(Dense(48, activation=\"relu\"))\n","        model.add(Dense(24, activation=\"relu\"))\n","        model.add(Dense(self.env.action_space.n))\n","        model.compile(loss=\"mean_squared_error\",\n","            optimizer=Adam(lr=self.learning_rate))\n","        return model\n","\n","    def act(self, state):\n","        self.epsilon *= self.epsilon_decay\n","        self.epsilon = max(self.epsilon_min, self.epsilon)\n","        if np.random.random() < self.epsilon:\n","            return self.env.action_space.sample()\n","        return np.argmax(self.model.predict(state)[0])\n","\n","    def remember(self, state, action, reward, new_state, done):\n","        self.memory.append([state, action, reward, new_state, done])\n","\n","    def replay(self):\n","        batch_size = 32\n","        if len(self.memory) < batch_size: \n","            return\n","\n","        samples = random.sample(self.memory, batch_size)\n","        for sample in samples:\n","            state, action, reward, new_state, done = sample\n","            target = self.target_model.predict(state)\n","            if done:\n","                target[0][action] = reward\n","            else:\n","                Q_future = max(self.target_model.predict(new_state)[0])\n","                target[0][action] = reward + Q_future * self.gamma\n","            self.model.fit(state, target, epochs=1, verbose=0)\n","\n","    def target_train(self):\n","        weights = self.model.get_weights()\n","        target_weights = self.target_model.get_weights()\n","        for i in range(len(target_weights)):\n","            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n","        self.target_model.set_weights(target_weights)\n","\n","    def save_model(self, fn):\n","        self.model.save(fn)\n","\n","def main():\n","    env     = gym.make(\"MountainCar-v0\")\n","    gamma   = 0.9\n","    epsilon = .95\n","\n","    trials  = 1000\n","    trial_len = 500\n","\n","    # updateTargetNetwork = 1000\n","    dqn_agent = DQN(env=env)\n","    steps = []\n","    for trial in range(trials):\n","        cur_state = env.reset().reshape(1,2)\n","        for step in range(trial_len):\n","            action = dqn_agent.act(cur_state)\n","            new_state, reward, done, _ = env.step(action)\n","\n","            # reward = reward if not done else -20\n","            new_state = new_state.reshape(1,2)\n","            dqn_agent.remember(cur_state, action, reward, new_state, done)\n","            \n","            dqn_agent.replay()       # internally iterates default (prediction) model\n","            dqn_agent.target_train() # iterates target model\n","\n","            cur_state = new_state\n","            if done:\n","                break\n","        if step >= 199:\n","            print(\"Failed to complete in trial {}\".format(trial))\n","            if step % 10 == 0:\n","                dqn_agent.save_model(\"trial-{}.model\".format(trial))\n","        else:\n","            print(\"Completed in {} trials\".format(trial))\n","            dqn_agent.save_model(\"success.model\")\n","            break\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","Failed to complete in trial 0\n","Failed to complete in trial 1\n","Failed to complete in trial 2\n","Failed to complete in trial 3\n","Failed to complete in trial 4\n","Failed to complete in trial 5\n","Failed to complete in trial 6\n","Failed to complete in trial 7\n","Failed to complete in trial 8\n","Failed to complete in trial 9\n","Failed to complete in trial 10\n","Failed to complete in trial 11\n","Failed to complete in trial 12\n","Failed to complete in trial 13\n","Failed to complete in trial 14\n","Failed to complete in trial 15\n","Failed to complete in trial 16\n","Failed to complete in trial 17\n","Failed to complete in trial 18\n","Failed to complete in trial 19\n","Failed to complete in trial 20\n","Failed to complete in trial 21\n","Failed to complete in trial 22\n","Failed to complete in trial 23\n","Failed to complete in trial 24\n","Failed to complete in trial 25\n","Failed to complete in trial 26\n","Failed to complete in trial 27\n","Failed to complete in trial 28\n","Failed to complete in trial 29\n","Failed to complete in trial 30\n","Failed to complete in trial 31\n","Failed to complete in trial 32\n","Failed to complete in trial 33\n","Failed to complete in trial 34\n","Failed to complete in trial 35\n","Failed to complete in trial 36\n","Failed to complete in trial 37\n","Failed to complete in trial 38\n","Failed to complete in trial 39\n","Failed to complete in trial 40\n","Failed to complete in trial 41\n","Failed to complete in trial 42\n","Failed to complete in trial 43\n","Failed to complete in trial 44\n","Failed to complete in trial 45\n","Completed in 46 trials\n"],"name":"stdout"}]},{"metadata":{"id":"gXLVWRffzsOz","colab_type":"text"},"cell_type":"markdown","source":["# Quiz\n","請將訓練好的model載入，並且用這個模型來決定action，並且其過程其畫出來。"]},{"metadata":{"id":"UEbE3iIIyUok","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}