{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 1. First Meeting with OpenAI Gym.ipynb","provenance":[{"file_id":"1CQ1jOgqXXxU7ykdWx-XSW_UQZlnX2_A-","timestamp":1544237210887}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"j9qqu6j7gQ4e","colab_type":"text"},"source":["# Lab 1. 在OpenAI上實作Q-Learning\n","歡迎大家來到增強式學習的世界！接下來，我們要利用OpenAI Gym這個套件，來實作以及驗證我們的演算法。\n","\n","在此處，因為我們使用的是Google CoLab，為了能夠方便在這個環境上面顯示，我們先安裝一些必要的套件。"]},{"cell_type":"code","metadata":{"id":"7gIwYTjZg52W","colab_type":"code","colab":{}},"source":["!apt-get install -y xvfb python-opengl xdpyinfo > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay xdpyinfo> /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nxrSpJChJKP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjVMtuzVh0Cy","colab_type":"text"},"source":["接著，我們產生一個Display的物件，並且設定在Google CoLab上的顯示參數。"]},{"cell_type":"code","metadata":{"id":"VsM4YgRziR2h","colab_type":"code","outputId":"f3d827fe-edf8-4a4f-ecce-7e83285aebd9","executionInfo":{"status":"ok","timestamp":1573454944734,"user_tz":-480,"elapsed":1204,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC1oV7VqFh-U1gNkoBIlNlcfSYUxNUtCLJX2K6A=s64","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["from IPython import display as ipythondisplay\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1024, 768))\n","display.start()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1021'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1024x768x24', ':1021'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"EEIF08yIkG1L","colab_type":"text"},"source":["設定完了顯示方式之後，我們就可以開始準備進入OepnAI Gym的世界了！OpenAI Gym 是一個提供許多測試環境的工具，讓大家有一個共同的環境可以測試自己的 RL 演算法，而不用花時間去搭建自己的測試環境。\n","\n","首先先引入必要的函式庫。主要的是gym，numpy和matplotlib分別用來做數學運算以及畫圖表的。"]},{"cell_type":"code","metadata":{"id":"7ZsN6AY4iS0i","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DK43viqDoE-A","colab_type":"text"},"source":["# 設定環境開始\n","\n","我們從一個最簡單的環境：CartPole開始。這是一個維持平衡棒子的遊戲。是一個增強學習中最知名的問題了。\n","\n","基本上，OpenAI Gym 提供了許許多多的環境，你可以在OpenAI的網站 ( https://gym.openai.com/envs/ )找到更多環境。\n","\n","在我們執行這個環境之前，我們可以先參閱官方網站上面的敘述 (https://github.com/openai/gym/wiki/CartPole-v0)。\n"]},{"cell_type":"code","metadata":{"id":"5j4iD6zwoJ1V","colab_type":"code","outputId":"d9af1f1b-5034-45a5-90dc-4e1ba403cb53","executionInfo":{"status":"ok","timestamp":1573454863574,"user_tz":-480,"elapsed":29,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC1oV7VqFh-U1gNkoBIlNlcfSYUxNUtCLJX2K6A=s64","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["env = gym.make(\"CartPole-v0\")\n","env.reset()\n","prev_screen = env.render(mode='rgb_array')\n","plt.imshow(prev_screen)\n","\n","rewards=0\n","for t in range(50):\n","  #亂數選一個動作\n","  action = env.action_space.sample()\n","  obs, reward, done, info = env.step(action)\n","  print(\"reward:\",reward,\"done:\",done)\n","  \n","  rewards+=reward\n","  #env.render()用來顯示結果\n","  screen = env.render(mode='rgb_array')\n","  #用matploblib畫出來\n","  plt.imshow(screen)\n","  ipythondisplay.clear_output(wait=True)\n","  ipythondisplay.display(plt.gcf())\n","    \n","  if done:\n","\t  break  \n","\n","ipythondisplay.clear_output(wait=True)\n","env.close()\n","\n","print(\"Total reward:\",rewards)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Total reward: 28.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNREXNdWa96IAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"TWhlp3jfLE-e","colab_type":"text"},"source":["首先你可以注意到，我們在選擇action的時候是用亂數選。這樣的選擇，得到的分數是24.0分。我們要以能夠突破這個分數為目標前進！"]},{"cell_type":"markdown","metadata":{"id":"nOPlRGaJqlqF","colab_type":"text"},"source":["# 環境狀態 (Observation)\n","\n","在gym中，observation就是我們在課堂上提到的state，也就是指環境裡面的狀態。\n","\n","在上面的程式中，我們可以看到env.step()提供環境狀態。env.step() 會回傳 4 個變數，分別是\n","1. observation (環境狀態)\n","2. reward (上一次 action 獲得的 reward )\n","3. done (判斷是否達到終止條件的變數)\n","4. info ( debug 用的資訊)\n","\n","呼叫 reset，整個環境就會重頭開始，此外 reset 會回傳一個初始的環境狀態。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lfn75QRNvZ_d","colab_type":"text"},"source":["# Space\n","\n","在前面的程式中，每次執行的(action)都是環境空間中隨機挑出的。\n","在 Gym 的環境中，有兩種空間：action_space 和observation_space。如果我們看一下類型的話，兩者都屬於Space類型，用來描述有效的動作還有環境的格式和範圍。"]},{"cell_type":"markdown","metadata":{"id":"a5bpZd6Jzgxz","colab_type":"text"},"source":["我們可以用以下的程式來看看到底有幾個action可以用，環境裡面又有幾個狀態："]},{"cell_type":"code","metadata":{"id":"I55dKU65ogA7","colab_type":"code","outputId":"6688f62b-204e-442b-a61e-6debc259e4fa","executionInfo":{"status":"ok","timestamp":1573454877794,"user_tz":-480,"elapsed":1024,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC1oV7VqFh-U1gNkoBIlNlcfSYUxNUtCLJX2K6A=s64","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["import gym\n","env = gym.make('CartPole-v0')\n","\n","print(env.action_space)\n","\n","print(env.observation_space)\n","print(env.observation_space.high)\n","print(env.observation_space.low)\n","env.close()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Discrete(2)\n","Box(4,)\n","[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n","[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2f15dyMlIuG0","colab_type":"text"},"source":["基本上，我們可以看到action_space 是一個Discrete類別的物件，他是一個{0,1,...,n-1}的非負整數的集合。在 CartPole-v0 例子中，動作空間為 {0,1}，代表往左往右。observation_space 則是一個Box類型的物件，用來表示一个 n 维的盒子，所以在上面我們會發現 observation 是一個長度4的數組，每個數組都有上下界（看起來就像一個盒子一般)。\n","\n","再次提醒，如果需要知道詳細的action以及observation所代表的意義請查這邊：https://github.com/openai/gym/wiki/CartPole-v0\n"]},{"cell_type":"markdown","metadata":{"id":"k0AqRmXszeHp","colab_type":"text"},"source":["# 惡搞一下\n","我們來試試看讓車子只往一邊跑"]},{"cell_type":"code","metadata":{"id":"xCjIrwlsyic9","colab_type":"code","outputId":"c9f5c370-70a1-4291-eac3-720b43473c1f","executionInfo":{"status":"ok","timestamp":1573454899438,"user_tz":-480,"elapsed":21,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC1oV7VqFh-U1gNkoBIlNlcfSYUxNUtCLJX2K6A=s64","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["import gym\n","from gym import spaces\n","  \n","env = gym.make('CartPole-v0')\n","env.action_space = spaces.Discrete(1) # Set it to only 1 elements {0}\n","  \n","observation = env.reset()\n","#用來在colab上畫圖的指令\n","prev_screen = env.render(mode='rgb_array')\n","plt.imshow(prev_screen)\n","\n","rewards = 0\n","for t in range(20):\n","  screen = env.render(mode='rgb_array')\n","  action = env.action_space.sample()\n","  observation, reward, done, info = env.step(action)\n","    \n","  rewards+=reward  \n","  \n","  #用matploblib畫出來\n","  plt.imshow(screen)\n","  ipythondisplay.clear_output(wait=True)\n","  ipythondisplay.display(plt.gcf())\n","  \n","  if done==True:\n","    break\n","      \n","ipythondisplay.clear_output(wait=True)\n","env.close()\n","print(\"Total reward:\",rewards)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Total reward: 10.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARoUlEQVR4nO3df6xcZ33n8fenSQhsQU1Cbi3XP9Zp\ncYXSqjj0bgiCP9Ig2hBVayqxKGlVLBTpplKQQEJtk67UBqmRWmlLtqjdqK6SxawoIS2gWFF2aWoi\nVfxBgg3G2AkpF3AUWyZ2IAkg1Ow6fPeP+zgM5jp37p07mfvceb+koznne86Z+T7K+JPjZ854UlVI\nkvrxM5NuQJK0PAa3JHXG4JakzhjcktQZg1uSOmNwS1JnxhbcSa5N8niS+SS3jOt1JGnaZBz3cSc5\nD/g34O3AMeCLwA1V9eiqv5gkTZlxXXFfCcxX1Ter6v8C9wA7x/RakjRVzh/T824CnhzYPga86VwH\nX3rppbVt27YxtSJJ/Tl69ChPP/10Fts3ruBeUpI5YA5g69at7N+/f1KtSNKaMzs7e85945oqOQ5s\nGdje3GovqqrdVTVbVbMzMzNjakOS1p9xBfcXge1JLkvyCuB6YO+YXkuSpspYpkqq6nSS9wGfBc4D\n7q6qI+N4LUmaNmOb466qB4AHxvX8kjSt/OakJHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmd\nMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOjPTTZUmO\nAt8HXgBOV9VskkuATwLbgKPAu6vqmdHalCSdsRpX3L9RVTuqarZt3wLsq6rtwL62LUlaJeOYKtkJ\n7Gnre4B3juE1JGlqjRrcBfxzkgNJ5lptQ1WdaOvfBjaM+BqSpAEjzXEDb62q40l+HngwydcGd1ZV\nJanFTmxBPwewdevWEduQpOkx0hV3VR1vjyeBzwBXAk8l2QjQHk+e49zdVTVbVbMzMzOjtCFJU2XF\nwZ3kZ5O85sw68JvAYWAvsKsdtgu4b9QmJUk/NspUyQbgM0nOPM8/VNX/SfJF4N4kNwJPAO8evU1J\n0hkrDu6q+ibwhkXq3wHeNkpTkqRz85uTktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCW\npM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmeWDO4k\ndyc5meTwQO2SJA8m+Xp7vLjVk+QjSeaTHEryxnE2L0nTaJgr7o8C155VuwXYV1XbgX1tG+AdwPa2\nzAF3rk6bkqQzlgzuqvpX4LtnlXcCe9r6HuCdA/WP1YIvABcl2bhazUqSVj7HvaGqTrT1bwMb2vom\n4MmB44612k9JMpdkf5L9p06dWmEbkjR9Rv5wsqoKqBWct7uqZqtqdmZmZtQ2JGlqrDS4nzozBdIe\nT7b6cWDLwHGbW02StEpWGtx7gV1tfRdw30D9Pe3ukquA5wamVCRJq+D8pQ5I8gngauDSJMeAPwP+\nArg3yY3AE8C72+EPANcB88APgfeOoWdJmmpLBndV3XCOXW9b5NgCbh61KUnSufnNSUnqjMEtSZ0x\nuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNb\nkjpjcEtSZwxuSeqMwS1JnVkyuJPcneRkksMDtduSHE9ysC3XDey7Ncl8kseT/Na4GpekaTXMFfdH\ngWsXqd9RVTva8gBAksuB64Ffaef8jyTnrVazkqQhgruq/hX47pDPtxO4p6qer6pvsfBr71eO0J8k\n6SyjzHG/L8mhNpVycattAp4cOOZYq/2UJHNJ9ifZf+rUqRHakKTpstLgvhP4JWAHcAL4q+U+QVXt\nrqrZqpqdmZlZYRuSNH1WFNxV9VRVvVBVPwL+nh9PhxwHtgwcurnVJEmrZEXBnWTjwObvAGfuONkL\nXJ/kwiSXAduBR0ZrUZI06PylDkjyCeBq4NIkx4A/A65OsgMo4ChwE0BVHUlyL/AocBq4uapeGE/r\nkjSdlgzuqrphkfJdL3H87cDtozQlSTo3vzkpSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1Jn\nlryPW5pWB3bf9FO1X5/7uwl0Iv0kr7glqTMGtyR1xuCWpM4Y3NIiFpvfltYKg1uSOmNwS1JnDG5p\nSN4KqLXC4Jakzhjc0ln8YFJrncEtDcFpEq0lSwZ3ki1JHkryaJIjSd7f6pckeTDJ19vjxa2eJB9J\nMp/kUJI3jnsQ0mrxals9GOaK+zTwwaq6HLgKuDnJ5cAtwL6q2g7sa9sA72Dh1923A3PAnavetSRN\nsSWDu6pOVNWX2vr3gceATcBOYE87bA/wzra+E/hYLfgCcFGSjaveuSRNqWXNcSfZBlwBPAxsqKoT\nbde3gQ1tfRPw5MBpx1rt7OeaS7I/yf5Tp04ts21Jml5DB3eSVwOfAj5QVd8b3FdVBdRyXriqdlfV\nbFXNzszMLOdU6WXlB5Naa4YK7iQXsBDaH6+qT7fyU2emQNrjyVY/DmwZOH1zq0mSVsEwd5UEuAt4\nrKo+PLBrL7Crre8C7huov6fdXXIV8NzAlIq0ZnlHiXoxzC/gvAX4feCrSQ622p8AfwHcm+RG4Ang\n3W3fA8B1wDzwQ+C9q9qxJE25JYO7qj4P5By737bI8QXcPGJfkqRz8JuT0kvwg0mtRQa3JHXG4Jbw\ng0n1xeCWpM4Y3JLUGYNbOgc/mNRaZXBLUmcMbknqjMGtqecdJeqNwS1JnTG4JakzBre0CO8o0Vpm\ncEtSZwxuTTU/mFSPDG5J6ozBLUmdMbils/jBpNY6g1uSOjPMjwVvSfJQkkeTHEny/la/LcnxJAfb\nct3AObcmmU/yeJLfGucAJGnaDPNjwaeBD1bVl5K8BjiQ5MG2746q+m+DBye5HLge+BXgF4B/SfLL\nVfXCajYujWqxO0qcJlEPlrzirqoTVfWltv594DFg00ucshO4p6qer6pvsfBr71euRrOSpGXOcSfZ\nBlwBPNxK70tyKMndSS5utU3AkwOnHeOlg16StAxDB3eSVwOfAj5QVd8D7gR+CdgBnAD+ajkvnGQu\nyf4k+0+dOrWcUyVpqg0V3EkuYCG0P15Vnwaoqqeq6oWq+hHw9/x4OuQ4sGXg9M2t9hOqandVzVbV\n7MzMzChjkJbNb0yqZ8PcVRLgLuCxqvrwQH3jwGG/Axxu63uB65NcmOQyYDvwyOq1LI2HH0yqF8Pc\nVfIW4PeBryY52Gp/AtyQZAdQwFHgJoCqOpLkXuBRFu5Iudk7SiRp9SwZ3FX1eSCL7HrgJc65Hbh9\nhL6ksXGaRL3zm5OS1BmDW5I6Y3BL+MGk+mJwS1JnDG5J6ozBraniHSVaDwxuSeqMwS1JnTG4NfW8\no0S9MbglqTMGt6aGH0xqvTC4JakzBrckdcbgVreSLGtZzOxNu1d8rjQpBrckdWaYH1KQ1oX7T8z9\nxPZvb9w9oU6k0XjFralw2237J92CtGoMbk2ts6/ApV4M82PBr0zySJKvJDmS5EOtflmSh5PMJ/lk\nkle0+oVte77t3zbeIUgr41SJejXMFffzwDVV9QZgB3BtkquAvwTuqKrXAc8AN7bjbwSeafU72nHS\nRC0W0rM3Gdzq0zA/FlzAD9rmBW0p4Brgd1t9D3AbcCews60D/BPwN0nSnkeaiIWQNqi1Pgx1V0mS\n84ADwOuAvwW+ATxbVafbIceATW19E/AkQFWdTvIc8Frg6XM9/4EDB7xXVmua70+tJUMFd1W9AOxI\nchHwGeD1o75wkjlgDmDr1q088cQToz6lpszLGab+hVEvt9nZ2XPuW9ZdJVX1LPAQ8GbgoiRngn8z\ncLytHwe2ALT9Pwd8Z5Hn2l1Vs1U1OzMzs5w2JGmqDXNXyUy70ibJq4C3A4+xEODvaoftAu5r63vb\nNm3/55zflqTVM8xUyUZgT5vn/hng3qq6P8mjwD1J/hz4MnBXO/4u4H8lmQe+C1w/hr4laWoNc1fJ\nIeCKRerfBK5cpP7vwH9Zle4kST/Fb05KUmcMbknqjMEtSZ3xn3VVt7xZSdPKK25J6ozBLUmdMbgl\nqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1Jlhfiz4\nlUkeSfKVJEeSfKjVP5rkW0kOtmVHqyfJR5LMJzmU5I3jHoQkTZNh/j3u54FrquoHSS4APp/kf7d9\nf1hV/3TW8e8AtrflTcCd7VGStAqWvOKuBT9omxe05aX+BfudwMfaeV8ALkqycfRWJUkw5Bx3kvOS\nHAROAg9W1cNt1+1tOuSOJBe22ibgyYHTj7WaJGkVDBXcVfVCVe0ANgNXJvlV4Fbg9cB/Ai4B/ng5\nL5xkLsn+JPtPnTq1zLYlaXot666SqnoWeAi4tqpOtOmQ54H/CVzZDjsObBk4bXOrnf1cu6tqtqpm\nZ2ZmVta9JE2hYe4qmUlyUVt/FfB24Gtn5q2TBHgncLidshd4T7u75Crguao6MZbuJWkKDXNXyUZg\nT5LzWAj6e6vq/iSfSzIDBDgI/EE7/gHgOmAe+CHw3tVvW5Km15LBXVWHgCsWqV9zjuMLuHn01iRJ\ni/Gbk5LUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknq\njMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTOpqkn3QJLvA49P\nuo8xuRR4etJNjMF6HRes37E5rr78x6qaWWzH+S93J+fweFXNTrqJcUiyfz2Obb2OC9bv2BzX+uFU\niSR1xuCWpM6sleDePekGxmi9jm29jgvW79gc1zqxJj6clCQNb61ccUuShjTx4E5ybZLHk8wnuWXS\n/SxXkruTnExyeKB2SZIHk3y9PV7c6knykTbWQ0neOLnOX1qSLUkeSvJokiNJ3t/qXY8tySuTPJLk\nK21cH2r1y5I83Pr/ZJJXtPqFbXu+7d82yf6XkuS8JF9Ocn/bXi/jOprkq0kOJtnfal2/F0cx0eBO\nch7wt8A7gMuBG5JcPsmeVuCjwLVn1W4B9lXVdmBf24aFcW5vyxxw58vU40qcBj5YVZcDVwE3t/82\nvY/teeCaqnoDsAO4NslVwF8Cd1TV64BngBvb8TcCz7T6He24tez9wGMD2+tlXAC/UVU7Bm796/29\nuHJVNbEFeDPw2YHtW4FbJ9nTCsexDTg8sP04sLGtb2ThPnWAvwNuWOy4tb4A9wFvX09jA/4D8CXg\nTSx8geP8Vn/xfQl8FnhzWz+/HZdJ936O8WxmIcCuAe4Hsh7G1Xo8Clx6Vm3dvBeXu0x6qmQT8OTA\n9rFW692GqjrR1r8NbGjrXY63/TX6CuBh1sHY2nTCQeAk8CDwDeDZqjrdDhns/cVxtf3PAa99eTse\n2n8H/gj4Udt+LetjXAAF/HOSA0nmWq379+JKrZVvTq5bVVVJur11J8mrgU8BH6iq7yV5cV+vY6uq\nF4AdSS4CPgO8fsItjSzJbwMnq+pAkqsn3c8YvLWqjif5eeDBJF8b3Nnre3GlJn3FfRzYMrC9udV6\n91SSjQDt8WSrdzXeJBewENofr6pPt/K6GBtAVT0LPMTCFMJFSc5cyAz2/uK42v6fA77zMrc6jLcA\n/znJUeAeFqZL/pr+xwVAVR1vjydZ+J/tlayj9+JyTTq4vwhsb598vwK4Htg74Z5Ww15gV1vfxcL8\n8Jn6e9qn3lcBzw38VW9NycKl9V3AY1X14YFdXY8tyUy70ibJq1iYt3+MhQB/Vzvs7HGdGe+7gM9V\nmzhdS6rq1qraXFXbWPhz9Lmq+j06HxdAkp9N8poz68BvAofp/L04kklPsgPXAf/Gwjzjf510Pyvo\n/xPACeD/sTCXdiMLc4X7gK8D/wJc0o4NC3fRfAP4KjA76f5fYlxvZWFe8RBwsC3X9T424NeAL7dx\nHQb+tNV/EXgEmAf+Ebiw1V/Ztufb/l+c9BiGGOPVwP3rZVxtDF9py5EzOdH7e3GUxW9OSlJnJj1V\nIklaJoNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTO/H+ygQzoskFQZAAAAABJRU5ErkJg\ngg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"nnAhhSj2KToe","colab_type":"text"},"source":["惡搞成功，如果一直讓滑車往左的話，一下子遊戲就停掉了。才得9分。"]},{"cell_type":"markdown","metadata":{"id":"EG5oaS4SGvCo","colab_type":"text"},"source":["# 來個工人智慧\n","\n","為了讓agent不會走得太無腦，再來引進一個簡單的策略：如果柱子向左傾（角度 < 0），則小車左移以維持平衡，否則右移。"]},{"cell_type":"code","metadata":{"id":"Wf7AcA9bG7ur","colab_type":"code","colab":{}},"source":["# 定義 policy\n","def choose_action(observation):\n","    pos, v, ang, rot = observation\n","    return 0 if ang < 0 else 1 # 柱子左傾則小車左移，否則右移 \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ClSkfGrRGyz0","colab_type":"text"},"source":["定義完了policy，我們將policy放入我們的程式中："]},{"cell_type":"code","metadata":{"id":"IVdPGcweINDi","colab_type":"code","outputId":"d427ef1b-02d7-4e64-b3c3-6f815b91628a","executionInfo":{"status":"ok","timestamp":1573454906268,"user_tz":-480,"elapsed":870,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC1oV7VqFh-U1gNkoBIlNlcfSYUxNUtCLJX2K6A=s64","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["env = gym.make('CartPole-v0')\n","\n","for i_episode in range(200):\n","    observation = env.reset()\n","    rewards = 0\n","    for t in range(250):\n","        #env.render()\n","\n","        action = choose_action(observation)\n","        observation, reward, done, info = env.step(action)\n","\n","        rewards += reward\n","\n","        if done:\n","            print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n","            break\n","\n","env.close()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 46 timesteps, total rewards 46.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 58 timesteps, total rewards 58.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 62 timesteps, total rewards 62.0\n","Episode finished after 54 timesteps, total rewards 54.0\n","Episode finished after 46 timesteps, total rewards 46.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 60 timesteps, total rewards 60.0\n","Episode finished after 46 timesteps, total rewards 46.0\n","Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 33 timesteps, total rewards 33.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 51 timesteps, total rewards 51.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 60 timesteps, total rewards 60.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 63 timesteps, total rewards 63.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 55 timesteps, total rewards 55.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 43 timesteps, total rewards 43.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 64 timesteps, total rewards 64.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 57 timesteps, total rewards 57.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 27 timesteps, total rewards 27.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 50 timesteps, total rewards 50.0\n","Episode finished after 46 timesteps, total rewards 46.0\n","Episode finished after 51 timesteps, total rewards 51.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 55 timesteps, total rewards 55.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 27 timesteps, total rewards 27.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 60 timesteps, total rewards 60.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 51 timesteps, total rewards 51.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 55 timesteps, total rewards 55.0\n","Episode finished after 51 timesteps, total rewards 51.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 57 timesteps, total rewards 57.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 27 timesteps, total rewards 27.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 48 timesteps, total rewards 48.0\n","Episode finished after 44 timesteps, total rewards 44.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 68 timesteps, total rewards 68.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 58 timesteps, total rewards 58.0\n","Episode finished after 54 timesteps, total rewards 54.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 54 timesteps, total rewards 54.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 59 timesteps, total rewards 59.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 40 timesteps, total rewards 40.0\n","Episode finished after 50 timesteps, total rewards 50.0\n","Episode finished after 49 timesteps, total rewards 49.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 57 timesteps, total rewards 57.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 41 timesteps, total rewards 41.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 56 timesteps, total rewards 56.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 48 timesteps, total rewards 48.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 58 timesteps, total rewards 58.0\n","Episode finished after 46 timesteps, total rewards 46.0\n","Episode finished after 36 timesteps, total rewards 36.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hP0I7a6EEFPG","colab_type":"text"},"source":["# 利用Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"1UbUNrmtEOC0","colab_type":"text"},"source":["先統整一下。我們的目標是學習到最佳 Q function，過程中以 ε-greedy 方法與 environment 互動，從中獲得 reward 以更新 Q table 裡的 Q value。先看一下基於 ε-greedy 的 policy 定義："]},{"cell_type":"code","metadata":{"id":"e6Zzj8Xezrgg","colab_type":"code","colab":{}},"source":["def choose_action(state, q_table, action_space, epsilon):\n","    if np.random.random_sample() < epsilon: # 有 ε 的機率會選擇隨機 action\n","        return action_space.sample() \n","    else: # 其他時間根據現有 policy 選擇 action，也就是在 Q table 裡目前 state 中，選擇擁有最大 Q value 的 action\n","        return np.argmax(q_table[state]) \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93cOvjNdEP9O","colab_type":"text"},"source":["再來是 state 的表示。在 CartPole 環境裡觀察到的 feature 都是連續值，不適合作為一個 table 的 index，因此要將一個區間一個區間的值包在一起用離散數值表示，也就是下面的 bucket："]},{"cell_type":"code","metadata":{"id":"VFQ-oF5QClVS","colab_type":"code","colab":{}},"source":["def get_state(observation, n_buckets, state_bounds):\n","    state = [0] * len(observation) \n","    for i, s in enumerate(observation): # 每個 feature 有不同的分配\n","        l, u = state_bounds[i][0], state_bounds[i][1] # 每個 feature 值的範圍上下限\n","        if s <= l: # 低於下限，分配為 0\n","            state[i] = 0\n","        elif s >= u: # 高於上限，分配為最大值\n","            state[i] = n_buckets[i] - 1\n","        else: # 範圍內，依比例分配\n","            state[i] = int(((s - l) / (u - l)) * n_buckets[i])\n","\n","    return tuple(state)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XEoig3KEfSv","colab_type":"text"},"source":["最後是學習。學習過程中為了方便收斂，一些參數像 ε 和 learning rate 會隨著時間遞減，也就是我們從大膽亂走，到越來越相信已經學到的經驗。"]},{"cell_type":"code","metadata":{"id":"NqTcvCqJEa0T","colab_type":"code","outputId":"ea338479-265f-4b3b-a558-21c9d5c906fd","executionInfo":{"status":"ok","timestamp":1573454918419,"user_tz":-480,"elapsed":1784,"user":{"displayName":"Hsu Orozco","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC1oV7VqFh-U1gNkoBIlNlcfSYUxNUtCLJX2K6A=s64","userId":"11529651156616225641"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import math\n","\n","env = gym.make('CartPole-v0')\n","\n","# 準備 Q table\n","## Environment 中各個 feature 的 bucket 分配數量\n","## 1 代表任何值皆表同一 state，也就是這個 feature 其實不重要\n","n_buckets = (1, 1, 6, 3)\n","\n","## Action 數量 \n","n_actions = env.action_space.n\n","\n","## State 範圍 \n","state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n","state_bounds[1] = [-0.5, 0.5]\n","state_bounds[3] = [-math.radians(50), math.radians(50)]\n","\n","## Q table，每個 state-action pair 存一值 \n","q_table = np.zeros(n_buckets + (n_actions,))\n","\n","# 一些學習過程中的參數\n","get_epsilon = lambda i: max(0.01, min(1, 1.0 - math.log10((i+1)/25)))  # epsilon-greedy; 隨時間遞減\n","get_lr = lambda i: max(0.01, min(0.5, 1.0 - math.log10((i+1)/25))) # learning rate; 隨時間遞減 \n","gamma = 0.99 # reward discount factor\n","\n","# Q-learning\n","for i_episode in range(200):\n","    epsilon = get_epsilon(i_episode)\n","    lr = get_lr(i_episode)\n","\n","    observation = env.reset()\n","    rewards = 0\n","    state = get_state(observation, n_buckets, state_bounds) # 將連續值轉成離散 \n","    for t in range(250):\n","        action = choose_action(state, q_table, env.action_space, epsilon)\n","        observation, reward, done, info = env.step(action)\n","\n","        rewards += reward\n","        next_state = get_state(observation, n_buckets, state_bounds)\n","\n","        # 更新 Q table\n","        q_next_max = np.amax(q_table[next_state]) # 進入下一個 state 後，預期得到最大總 reward\n","        q_table[state + (action,)] += lr * (reward + gamma * q_next_max - q_table[state + (action,)]) # 就是那個公式\n","\n","        # 前進下一 state \n","        state = next_state\n","\n","        if done:\n","            print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n","            break\n","\n","env.close()\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Episode finished after 20 timesteps, total rewards 20.0\n","Episode finished after 48 timesteps, total rewards 48.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 15 timesteps, total rewards 15.0\n","Episode finished after 52 timesteps, total rewards 52.0\n","Episode finished after 21 timesteps, total rewards 21.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 22 timesteps, total rewards 22.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 24 timesteps, total rewards 24.0\n","Episode finished after 39 timesteps, total rewards 39.0\n","Episode finished after 22 timesteps, total rewards 22.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 14 timesteps, total rewards 14.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 28 timesteps, total rewards 28.0\n","Episode finished after 47 timesteps, total rewards 47.0\n","Episode finished after 23 timesteps, total rewards 23.0\n","Episode finished after 42 timesteps, total rewards 42.0\n","Episode finished after 27 timesteps, total rewards 27.0\n","Episode finished after 17 timesteps, total rewards 17.0\n","Episode finished after 17 timesteps, total rewards 17.0\n","Episode finished after 21 timesteps, total rewards 21.0\n","Episode finished after 24 timesteps, total rewards 24.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 15 timesteps, total rewards 15.0\n","Episode finished after 19 timesteps, total rewards 19.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 28 timesteps, total rewards 28.0\n","Episode finished after 15 timesteps, total rewards 15.0\n","Episode finished after 23 timesteps, total rewards 23.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 14 timesteps, total rewards 14.0\n","Episode finished after 14 timesteps, total rewards 14.0\n","Episode finished after 10 timesteps, total rewards 10.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 18 timesteps, total rewards 18.0\n","Episode finished after 19 timesteps, total rewards 19.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 30 timesteps, total rewards 30.0\n","Episode finished after 17 timesteps, total rewards 17.0\n","Episode finished after 17 timesteps, total rewards 17.0\n","Episode finished after 22 timesteps, total rewards 22.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 12 timesteps, total rewards 12.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 33 timesteps, total rewards 33.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 12 timesteps, total rewards 12.0\n","Episode finished after 12 timesteps, total rewards 12.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 20 timesteps, total rewards 20.0\n","Episode finished after 18 timesteps, total rewards 18.0\n","Episode finished after 25 timesteps, total rewards 25.0\n","Episode finished after 26 timesteps, total rewards 26.0\n","Episode finished after 31 timesteps, total rewards 31.0\n","Episode finished after 36 timesteps, total rewards 36.0\n","Episode finished after 22 timesteps, total rewards 22.0\n","Episode finished after 60 timesteps, total rewards 60.0\n","Episode finished after 30 timesteps, total rewards 30.0\n","Episode finished after 21 timesteps, total rewards 21.0\n","Episode finished after 14 timesteps, total rewards 14.0\n","Episode finished after 8 timesteps, total rewards 8.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 14 timesteps, total rewards 14.0\n","Episode finished after 24 timesteps, total rewards 24.0\n","Episode finished after 27 timesteps, total rewards 27.0\n","Episode finished after 44 timesteps, total rewards 44.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 20 timesteps, total rewards 20.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 18 timesteps, total rewards 18.0\n","Episode finished after 9 timesteps, total rewards 9.0\n","Episode finished after 17 timesteps, total rewards 17.0\n","Episode finished after 15 timesteps, total rewards 15.0\n","Episode finished after 68 timesteps, total rewards 68.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 38 timesteps, total rewards 38.0\n","Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 18 timesteps, total rewards 18.0\n","Episode finished after 35 timesteps, total rewards 35.0\n","Episode finished after 20 timesteps, total rewards 20.0\n","Episode finished after 32 timesteps, total rewards 32.0\n","Episode finished after 34 timesteps, total rewards 34.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 13 timesteps, total rewards 13.0\n","Episode finished after 24 timesteps, total rewards 24.0\n","Episode finished after 101 timesteps, total rewards 101.0\n","Episode finished after 28 timesteps, total rewards 28.0\n","Episode finished after 78 timesteps, total rewards 78.0\n","Episode finished after 45 timesteps, total rewards 45.0\n","Episode finished after 30 timesteps, total rewards 30.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 37 timesteps, total rewards 37.0\n","Episode finished after 98 timesteps, total rewards 98.0\n","Episode finished after 70 timesteps, total rewards 70.0\n","Episode finished after 14 timesteps, total rewards 14.0\n","Episode finished after 44 timesteps, total rewards 44.0\n","Episode finished after 142 timesteps, total rewards 142.0\n","Episode finished after 12 timesteps, total rewards 12.0\n","Episode finished after 118 timesteps, total rewards 118.0\n","Episode finished after 9 timesteps, total rewards 9.0\n","Episode finished after 48 timesteps, total rewards 48.0\n","Episode finished after 61 timesteps, total rewards 61.0\n","Episode finished after 10 timesteps, total rewards 10.0\n","Episode finished after 139 timesteps, total rewards 139.0\n","Episode finished after 180 timesteps, total rewards 180.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 150 timesteps, total rewards 150.0\n","Episode finished after 28 timesteps, total rewards 28.0\n","Episode finished after 92 timesteps, total rewards 92.0\n","Episode finished after 16 timesteps, total rewards 16.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 19 timesteps, total rewards 19.0\n","Episode finished after 10 timesteps, total rewards 10.0\n","Episode finished after 53 timesteps, total rewards 53.0\n","Episode finished after 11 timesteps, total rewards 11.0\n","Episode finished after 12 timesteps, total rewards 12.0\n","Episode finished after 24 timesteps, total rewards 24.0\n","Episode finished after 108 timesteps, total rewards 108.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 92 timesteps, total rewards 92.0\n","Episode finished after 18 timesteps, total rewards 18.0\n","Episode finished after 43 timesteps, total rewards 43.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n","Episode finished after 200 timesteps, total rewards 200.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CZPekPl5MCIL","colab_type":"text"},"source":["可以看到在訓練後期，agent 已經學會如何最大化自己的 reward，也就是維持住小車上的棒子了。\n","\n","這邊有幾個Magic number，包括如何分bucket (哪些feature是重要的)、 state 的上下限、還有參數設定都是需要多試幾次來調整，來達到最好結果的。這邊為了展示給大家看，所以我就放上了前人試過的參數。大家也可以試試看自己從0到1的設定這些參數。"]},{"cell_type":"markdown","metadata":{"id":"CfQooH6MPait","colab_type":"text"},"source":["# Quiz 1: \n","既然都求出Q-Table了，請完成一個程式畫出滑車如何利用Q-Table來保持平衡吧！"]},{"cell_type":"code","metadata":{"id":"9riuZbu0FRoz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}